{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.infer import SVI, Trace_ELBO, autoguide\n",
    "import torch\n",
    "from torch.nn.functional import softplus\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import anndata as ann\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from numpyro.distributions import TransformedDistribution, transforms\n",
    "from torch.distributions.transforms import SoftmaxTransform\n",
    "from scipy.stats import spearmanr\n",
    "import torch\n",
    "import scanpy as sc\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "data = ann.read_h5ad(\"/mnt/storage/thien/projectdata/GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\\n,\n",
    "n_obs = 100\n",
    "n_features1 = 20\n",
    "n_features2 = 30\n",
    "n_factors = 20\n",
    "\n",
    "torch.manual_seed(2024)\n",
    "Z_in = torch.randn(n_obs, n_factors)\n",
    "W1_in = torch.randn(n_features1, n_factors)\n",
    "W2_in = torch.randn(n_features2, n_factors)\n",
    "\n",
    "# create observated values from the simulated factor and weight matrix with some random noise\\n,\n",
    "Y1 = torch.matmul(Z_in, W1_in.t()) + 0.2 * torch.randn(n_obs, n_features1)\n",
    "Y2 = torch.matmul(Z_in, W2_in.t()) + 0.2 * torch.randn(n_obs, n_features2)\n",
    "#print(Y.shape)\\n,\n",
    "#print(Y)\\n,\n",
    "\n",
    "Y = torch.cat((Y1.T, Y2.T)).T\n",
    "#train_size = int(0.2*Y.shape[0])\n",
    "#train_data = Y[:train_size,:]\n",
    "#test_data = Y[train_size:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, Y1, Y2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.Y1 = Y1\n",
    "        self.Y2 = Y2\n",
    "        self.K = K\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        self.num_features1 = self.Y1.shape[1]\n",
    "        self.num_features2 = self.Y2.shape[1]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))               \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1.t())\n",
    "        Y2_hat = torch.matmul(Z, W2.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoDelta(self.model)\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        for j in range(num_iterations):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])              \n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide([Y1])\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, train_data, test_data, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = train_data[:,:n_features1]\n",
    "        self.Y2 = train_data[:,n_features1:]\n",
    "        self.K = K\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))               \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1.t())\n",
    "        Y2_hat = torch.matmul(Z, W2.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoNormal(self.model)\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, train_data, test_data, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = train_data[:,:n_features1]\n",
    "        self.Y2 = train_data[:,n_features1:]\n",
    "        self.K = K\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor([10 ** -1]), torch.tensor([10 ** -1])))\n",
    "                W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1))\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor([10 ** -1]), torch.tensor([10 ** -1])))\n",
    "                W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2))          \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t())\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor([10 ** -14]), torch.tensor([10 ** -14])))\n",
    "                    print(precision1)\n",
    "                    scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1./precision1))\n",
    "                    print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor([10 ** -14]), torch.tensor([10 ** -14])))\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./precision2))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        #torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        #pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, train_data, test_data, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = train_data[:,:n_features1]\n",
    "        self.Y2 = train_data[:,n_features1:]\n",
    "        self.K = K\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14)))\n",
    "                W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1))\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14)))\n",
    "                W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2))          \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t())\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "                    #print(precision1)\n",
    "                    scale = pyro.sample(\"scale1\", pyro.distributions.LogNormal(0., 1./(precision1)))\n",
    "                    #print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./(precision2)))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        #guide = autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"]))\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, dat, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        Data_Loader_dat = torch.utils.data.DataLoader(dat, batch_size=128)\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = dat[:,:n_features1]\n",
    "        self.Y2 = dat[:,n_features1:]\n",
    "        self.K = K\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.)).to(device)\n",
    "                alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14))).to(device)\n",
    "                W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1)).to(device)\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.)).to(device)\n",
    "                alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14))).to(device)\n",
    "                W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2)).to(device)\n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.)).to(device)\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t()).to(device)\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t()).to(device)\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if self.Y1 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y1 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0)\n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.))).to(device)\n",
    "                    #print(precision1)\n",
    "                    scale = pyro.sample(\"scale1\", pyro.distributions.LogNormal(0., 1./(precision1))).to(device)\n",
    "                    #print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1).to(device)\n",
    "                    #Y1_sampled = pyro.distributions.Normal(Y1_hat, scale)\n",
    "                    #pyro.sample(\"obs1\", pyro.distributions.TransformedDistribution(Y1_sampled, SoftmaxTransform()), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if self.Y2 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y2 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.))).to(device)\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./(precision2))).to(device)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2).to(device)\n",
    "                    #Y2_sampled = pyro.distributions.Normal(Y2_hat, scale)\n",
    "                    #pyro.sample(\"obs2\", pyro.distributions.TransformedDistribution(Y2_sampled, SoftmaxTransform()), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        #guide = autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"]))\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        for batch in dat:\n",
    "        # initialize stochastic variational inference\n",
    "            svi = SVI(\n",
    "                model = self.model,\n",
    "                guide = guide,\n",
    "                optim = optimizer,\n",
    "                loss = elbo\n",
    "            )\n",
    "            \n",
    "            num_iterations = 2000\n",
    "            train_loss = []\n",
    "            for j in range(num_iterations):\n",
    "            #for j in enumerate(self.train_dataloader):\n",
    "                # calculate the loss and take a gradient step\n",
    "                loss = svi.step()\n",
    "\n",
    "                train_loss.append(loss/self.Y1.shape[0])\n",
    "                #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "                if j % 20 == 0:\n",
    "                    print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "                \n",
    "                #with torch.no_grad():  # for logging only\n",
    "                    #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                    #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "                #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "            # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "            \n",
    "        return train_loss.detach().cpu().numpy(), map_estimates.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cite_GEX = sc.read_h5ad(\"/mnt/storage/anna/data/Cite_GEX_preprocessed.h5ad\")\n",
    "Cite_ADT = sc.read_h5ad(\"/mnt/storage/anna/data/Cite_ADT_preprocessed.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "Y = ad.concat([Cite_ADT, Cite_GEX], axis = 1, merge=\"same\")\n",
    "Y = torch.tensor(Y.X.A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([128, 2835])\n",
      "torch.Size([21, 2835])\n"
     ]
    }
   ],
   "source": [
    "for batch in Data_Loader_Y:\n",
    "    print(batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[:10000,:]\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m FA_model \u001b[38;5;241m=\u001b[39m \u001b[43mFA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mData_Loader_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m134\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2701\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m losses, estimates \u001b[38;5;241m=\u001b[39m FA_model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36mFA.__init__\u001b[0;34m(self, dat, n_features1, n_features2, K)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features1 \u001b[38;5;241m=\u001b[39m n_features1\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features2 \u001b[38;5;241m=\u001b[39m n_features2\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY1 \u001b[38;5;241m=\u001b[39m \u001b[43mdat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_features1\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY2 \u001b[38;5;241m=\u001b[39m dat[:,n_features1:]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m=\u001b[39m K\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "n_factors = 5\n",
    "FA_model = FA(Data_Loader_Y,134, 2701, n_factors)\n",
    "losses, estimates = FA_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'obs1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m correlations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_factors):\n\u001b[0;32m----> 3\u001b[0m     correlations\u001b[38;5;241m.\u001b[39mappend(spearmanr(Y1[:,latent_factor], \u001b[43mestimates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[:,latent_factor], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      4\u001b[0m correlations\n",
      "\u001b[0;31mKeyError\u001b[0m: 'obs1'"
     ]
    }
   ],
   "source": [
    "correlations = []\n",
    "for latent_factor in range(n_factors):\n",
    "    correlations.append(spearmanr(W1_in[:,latent_factor], estimates[\"w1_hat\"].detach().numpy()[:,latent_factor], axis=0))\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\\n,\n",
    "n_obs = 100\n",
    "n_features1 = 20\n",
    "n_features2 = 30\n",
    "n_factors = 20\n",
    "\n",
    "torch.manual_seed(2024)\n",
    "Z_in = torch.randn(n_obs, n_factors)\n",
    "W1_in = torch.randn(n_features1, n_factors)\n",
    "W2_in = torch.randn(n_features2, n_factors)\n",
    "\n",
    "with pyro.plate(\"features1\", n_features1):\n",
    "    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "    residuals1 = pyro.sample(\"scale1\", pyro.distributions.Normal(0., 1./(precision1)))\n",
    "with pyro.plate(\"features2\", n_features2):\n",
    "    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "    residuals2 = pyro.sample(\"scale2\", pyro.distributions.Normal(0., 1./(precision2)))\n",
    "\n",
    "# create observated values from the simulated factor and weight matrix with some random noise\\n,\n",
    "Y1 = torch.matmul(Z_in, W1_in.t()) + residuals1\n",
    "Y2 = torch.matmul(Z_in, W2_in.t()) + residuals2\n",
    "#print(Y.shape)\\n,\n",
    "#print(Y)\\n,\n",
    "\n",
    "Y = torch.cat((Y1.T, Y2.T)).T\n",
    "#train_size = int(0.2*Y.shape[0])\n",
    "#train_data = Y[:train_size,:]\n",
    "#test_data = Y[train_size:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "saved_model_dict = torch.load(\"/mnt/storage/thien/FA_model.pt\")\n",
    "FA_model.load_state_dict(saved_model_dict['model'])\n",
    "guide = saved_model_dict['guide']\n",
    "pyro.get_param_store().load(\"/mnt/storage/thien/FA_model_params.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = []\n",
    "for latent_factor in range(n_factors):\n",
    "    correlations.append(spearmanr(W1_in[:,latent_factor], estimates[\"w1_hat\"].detach().numpy()[:,latent_factor], axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
