{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.infer import SVI, Trace_ELBO, autoguide\n",
    "import torch\n",
    "from torch.nn.functional import softplus\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import anndata as ann\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from numpyro.distributions import TransformedDistribution, transforms\n",
    "from torch.distributions.transforms import SoftmaxTransform\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import torch\n",
    "import scanpy as sc\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "data = ann.read_h5ad(\"/mnt/storage/thien/projectdata/GSE194122_openproblems_neurips2021_cite_BMMC_processed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\\n,\n",
    "n_obs = 100\n",
    "n_features1 = 20\n",
    "n_features2 = 30\n",
    "n_factors = 20\n",
    "\n",
    "random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "np.random.seed(2024)\n",
    "\n",
    "percentage = 0.8 # percentage of values set to zero\n",
    "\n",
    "Z_in = torch.randn(n_obs, n_factors)\n",
    "W1_in = torch.randn(n_features1, n_factors)\n",
    "W2_in = torch.randn(n_features2, n_factors)\n",
    "\n",
    "# create observated values from the simulated factor and weight matrix with some random noise\\n,\n",
    "Y1 = torch.matmul(Z_in, W1_in.t()) #+ 0.2 * torch.randn(n_obs, n_features1)\n",
    "Y2 = torch.matmul(Z_in, W2_in.t()) #+ 0.2 * torch.randn(n_obs, n_features2)\n",
    "\n",
    "#try creating shared factors which are correlating\n",
    "Y1[:,:9] = torch.sort(Y1[:,:9]).values\n",
    "Y2[:,:9] = torch.sort(Y2[:,:9]).values\n",
    "\n",
    "#introducing NANs/0 into dataframe\n",
    "#Y1_indices = np.random.choice(Y1.shape[1]*Y1.shape[0], replace=False, size=int(Y1.shape[1]*Y1.shape[0]*percentage))\n",
    "#Y1[np.unravel_index(Y1_indices, Y1.shape)] = 0 \n",
    "\n",
    "#Y2_indices = np.random.choice(Y2.shape[1]*Y2.shape[0], replace=False, size=int(Y2.shape[1]*Y2.shape[0]*percentage))\n",
    "#Y2[np.unravel_index(Y2_indices, Y2.shape)] = 0 \n",
    "\n",
    "\n",
    "Y = torch.cat((Y1.T, Y2.T)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, Y1, Y2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.Y1 = Y1\n",
    "        self.Y2 = Y2\n",
    "        self.K = K\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        self.num_features1 = self.Y1.shape[1]\n",
    "        self.num_features2 = self.Y2.shape[1]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))               \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1.t())\n",
    "        Y2_hat = torch.matmul(Z, W2.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoDelta(self.model)\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        for j in range(num_iterations):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])              \n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide([Y1])\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, train_data, test_data, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = train_data[:,:n_features1]\n",
    "        self.Y2 = train_data[:,n_features1:]\n",
    "        self.K = K\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))               \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1.t())\n",
    "        Y2_hat = torch.matmul(Z, W2.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1.))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoNormal(self.model)\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, train_data, test_data, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = train_data[:,:n_features1]\n",
    "        self.Y2 = train_data[:,n_features1:]\n",
    "        self.K = K\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor([10 ** -1]), torch.tensor([10 ** -1])))\n",
    "                W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1))\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor([10 ** -1]), torch.tensor([10 ** -1])))\n",
    "                W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2))          \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t())\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor([10 ** -14]), torch.tensor([10 ** -14])))\n",
    "                    print(precision1)\n",
    "                    scale = pyro.sample(\"scale\", pyro.distributions.LogNormal(0., 1./precision1))\n",
    "                    print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor([10 ** -14]), torch.tensor([10 ** -14])))\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./precision2))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        #torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        #pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, train_data, test_data, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = train_data[:,:n_features1]\n",
    "        self.Y2 = train_data[:,n_features1:]\n",
    "        self.K = K\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14)))\n",
    "                W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1))\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.))\n",
    "                alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14)))\n",
    "                W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2))          \n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t())\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "                    #print(precision1)\n",
    "                    scale = pyro.sample(\"scale1\", pyro.distributions.LogNormal(0., 1./(precision1)))\n",
    "                    #print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if data is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if data is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./(precision2)))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        #guide = autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"]))\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "        # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "        \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, dat, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = dat[:,:n_features1]\n",
    "        self.Y2 = dat[:,n_features1:]\n",
    "        self.K = K\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W1_hat = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                #theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.))\n",
    "                #alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "                #W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1))\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                W2_hat = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                #theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.))\n",
    "                #alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "                #W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2))\n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.))\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t())\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t())\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if self.Y1 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y1 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0)\n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(20.), torch.tensor(20.)))\n",
    "                    #precision1 = 0.6\n",
    "                    #print(precision1)\n",
    "                    scale = pyro.sample(\"scale1\", pyro.distributions.LogNormal(0., 1./(precision1)))\n",
    "                    #print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1)\n",
    "                    #Y1_sampled = pyro.distributions.Normal(Y1_hat, scale)\n",
    "                    #pyro.sample(\"obs1\", pyro.distributions.TransformedDistribution(Y1_sampled, SoftmaxTransform()), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if self.Y2 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y2 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(20.), torch.tensor(20.)))\n",
    "                    #precision2 = 0.6\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./(precision2)))\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2)\n",
    "                    #Y2_sampled = pyro.distributions.Normal(Y2_hat, scale)\n",
    "                    #pyro.sample(\"obs2\", pyro.distributions.TransformedDistribution(Y2_sampled, SoftmaxTransform()), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        #guide = autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"]))\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 5000\n",
    "        train_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "            # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "            \n",
    "        return train_loss, map_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, dat, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        Data_Loader_dat = torch.utils.data.DataLoader(dat, batch_size=128)\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = dat[:,:n_features1]\n",
    "        self.Y2 = dat[:,n_features1:]\n",
    "        self.K = K\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.sample_plate = pyro.plate(\"sample\", self.num_samples)\n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.)).to(device)\n",
    "                alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14))).to(device)\n",
    "                W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1)).to(device)\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.)).to(device)\n",
    "                alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14))).to(device)\n",
    "                W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2)).to(device)\n",
    "                \n",
    "            with self.sample_plate:\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.)).to(device)\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t()).to(device)\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t()).to(device)\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if self.Y1 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y1 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0)\n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.))).to(device)\n",
    "                    #print(precision1)\n",
    "                    scale = pyro.sample(\"scale1\", pyro.distributions.LogNormal(0., 1./(precision1))).to(device)\n",
    "                    #print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1).to(device)\n",
    "                    #Y1_sampled = pyro.distributions.Normal(Y1_hat, scale)\n",
    "                    #pyro.sample(\"obs1\", pyro.distributions.TransformedDistribution(Y1_sampled, SoftmaxTransform()), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if self.Y2 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y2 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.))).to(device)\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./(precision2))).to(device)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2).to(device)\n",
    "                    #Y2_sampled = pyro.distributions.Normal(Y2_hat, scale)\n",
    "                    #pyro.sample(\"obs2\", pyro.distributions.TransformedDistribution(Y2_sampled, SoftmaxTransform()), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        #guide = autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"]))\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "            # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "            \n",
    "        return train_loss.detach().cpu().numpy(), map_estimates.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FA(PyroModule):\n",
    "    def __init__(self, dat, n_features1, n_features2, K):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Y: Tensor (Samples x Features)\n",
    "            K: Number of Latent Factors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        pyro.clear_param_store()\n",
    "        \n",
    "        # data\n",
    "        self.num_features1 = n_features1\n",
    "        self.num_features2 = n_features2\n",
    "        self.Y1 = dat[:,:n_features1]\n",
    "        self.Y2 = dat[:,n_features1:]\n",
    "        self.K = K\n",
    "        \n",
    "        self.num_samples = self.Y1.shape[0]\n",
    "        \n",
    "        self.feature_plate1 = pyro.plate(\"feature1\", self.num_features1)\n",
    "        self.feature_plate2 = pyro.plate(\"feature2\", self.num_features2)\n",
    "        self.latent_factor_plate = pyro.plate(\"latent factors\", self.K)\n",
    "        \n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        how to generate a matrix\n",
    "        \"\"\"\n",
    "        with self.latent_factor_plate:\n",
    "            with self.feature_plate1:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W1 = pyro.sample(\"W1\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta1 = pyro.sample(\"theta1\", pyro.distributions.Beta(1., 1.)).to(device)\n",
    "                alpha1 = pyro.sample(\"alpha1\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14))).to(device)\n",
    "                W1_hat = pyro.sample(\"w1_hat\", pyro.distributions.Normal(0., 1./alpha1))*pyro.sample(\"s1\", pyro.distributions.Bernoulli(theta1)).to(device)\n",
    "\n",
    "            with self.feature_plate2:\n",
    "                # sample weight matrix with Normal prior distribution\n",
    "                #W2 = pyro.sample(\"W2\", pyro.distributions.Normal(0., 1.))  \n",
    "                theta2 = pyro.sample(\"theta2\", pyro.distributions.Beta(1., 1.)).to(device)\n",
    "                alpha2 = pyro.sample(\"alpha2\", pyro.distributions.Gamma(torch.tensor(10**-14), torch.tensor(10**-14))).to(device)\n",
    "                W2_hat = pyro.sample(\"w2_hat\",pyro.distributions.Normal(0., 1./alpha2))*pyro.sample(\"s2\",pyro.distributions.Bernoulli(theta2)).to(device)\n",
    "            batch_size = []\n",
    "            with pyro.plate(\"my_plate\", self.num_samples, 128) as ind:\n",
    "                batch_size.append(ind)\n",
    "            batch_size = torch.LongTensor(batch_size[0])\n",
    "            with pyro.plate(\"sample\", subsample=batch_size):\n",
    "                # sample factor matrix with Normal prior distribution\n",
    "                Z = pyro.sample(\"Z\", pyro.distributions.Normal(0., 1.)).to(device)\n",
    "        \n",
    "        # estimate for Y\n",
    "        Y1_hat = torch.matmul(Z, W1_hat.t()).to(device)\n",
    "        Y2_hat = torch.matmul(Z, W2_hat.t()).to(device)\n",
    "        \n",
    "        with pyro.plate(\"feature1_\", self.Y1.shape[1]), pyro.plate(\"sample_\", self.Y1.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y1, dtype=torch.bool)\n",
    "            if self.Y1 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y1))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y1 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y1 = torch.nan_to_num(self.Y1, nan=0)\n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.))).to(device)\n",
    "                    #print(precision1)\n",
    "                    scale = pyro.sample(\"scale1\", pyro.distributions.LogNormal(0., 1./(precision1))).to(device)\n",
    "                    #print(scale)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs1\", pyro.distributions.Normal(Y1_hat, scale), obs=self.Y1).to(device)\n",
    "                    #Y1_sampled = pyro.distributions.Normal(Y1_hat, scale)\n",
    "                    #pyro.sample(\"obs1\", pyro.distributions.TransformedDistribution(Y1_sampled, SoftmaxTransform()), obs=self.Y1)\n",
    "\n",
    "\n",
    "        with pyro.plate(\"feature2_\", self.Y2.shape[1]), pyro.plate(\"sample2_\", self.Y2.shape[0]):\n",
    "            # masking the NA values such that they are not considered in the distributions\n",
    "            obs_mask = torch.ones_like(self.Y2, dtype=torch.bool)\n",
    "            if self.Y2 is not None:\n",
    "                obs_mask = torch.logical_not(torch.isnan(self.Y2))\n",
    "            with pyro.poutine.mask(mask=obs_mask):\n",
    "                if self.Y2 is not None:\n",
    "                    # a valid value for the NAs has to be defined even though these samples will be ignored later\n",
    "                    self.Y2 = torch.nan_to_num(self.Y2, nan=0) \n",
    "            \n",
    "                    # sample scale parameter for each feature-sample pair with LogNormal prior (has to be positive)\n",
    "                    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.))).to(device)\n",
    "                    scale = pyro.sample(\"scale2\", pyro.distributions.LogNormal(0., 1./(precision2))).to(device)\n",
    "                    # compare sampled estimation to the true observation Y\n",
    "                    pyro.sample(\"obs2\", pyro.distributions.Normal(Y2_hat, scale), obs=self.Y2).to(device)\n",
    "                    #Y2_sampled = pyro.distributions.Normal(Y2_hat, scale)\n",
    "                    #pyro.sample(\"obs2\", pyro.distributions.TransformedDistribution(Y2_sampled, SoftmaxTransform()), obs=self.Y2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        # set training parameters\n",
    "        optimizer = pyro.optim.Adam({\"lr\": 0.02})\n",
    "        elbo = Trace_ELBO()\n",
    "        #guide = autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"]))\n",
    "        guide = autoguide.AutoGuideList(self.model)\n",
    "        guide.append(autoguide.AutoNormal(pyro.poutine.block(self.model, hide=['s1', \"s2\"])))\n",
    "        guide.append(autoguide.AutoDiscreteParallel(pyro.poutine.block(self.model, expose=[\"s1\", \"s2\"])))\n",
    "        \n",
    "        # initialize stochastic variational inference\n",
    "        svi = SVI(\n",
    "            model = self.model,\n",
    "            guide = guide,\n",
    "            optim = optimizer,\n",
    "            loss = elbo\n",
    "        )\n",
    "        \n",
    "        num_iterations = 2000\n",
    "        train_loss = []\n",
    "        for j in range(num_iterations):\n",
    "        #for j in enumerate(self.train_dataloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "            loss = svi.step()\n",
    "\n",
    "            train_loss.append(loss/self.Y1.shape[0])\n",
    "            #    test_loss.append(elbo.loss(self.model, guide, test_data))\n",
    "            if j % 200 == 0:\n",
    "                print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / self.Y1.shape[0]))\n",
    "            \n",
    "            #with torch.no_grad():  # for logging only\n",
    "                #train_loss = elbo.loss(self.model, guide, self.train_data) # or average over batch_loss\n",
    "                #test_loss = elbo.loss(self.model, guide, self.test_data)\n",
    "            #print(train_loss, test_loss)\n",
    "        torch.save({\"model\": self.state_dict(), \"guide\" : guide}, \"/mnt/storage/thien/FA_model.pt\")\n",
    "        pyro.get_param_store().save(\"/mnt/storage/thien/FA_model_params.pt\")\n",
    "\n",
    "            # Obtain maximum a posteriori estimates for W and Z\n",
    "        map_estimates = guide(Y)\n",
    "            \n",
    "        return train_loss.detach().cpu().numpy(), map_estimates.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cite_GEX = sc.read_h5ad(\"/mnt/storage/anna/data/Cite_GEX_preprocessed.h5ad\")\n",
    "Cite_ADT = sc.read_h5ad(\"/mnt/storage/anna/data/Cite_ADT_preprocessed.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/anndata/_core/anndata.py:1840: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "Y = ad.concat([Cite_ADT, Cite_GEX], axis = 1, merge=\"same\")\n",
    "Y = torch.tensor(Y.X.A)\n",
    "Y_DataLoader = torch.utils.data.DataLoader(Y, batch_size=128) #set up the training data in the right format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 305.8013\n",
      "[iteration 0201] loss: 103.7812\n",
      "[iteration 0401] loss: 102.0967\n",
      "[iteration 0601] loss: 101.2018\n",
      "[iteration 0801] loss: 98.6022\n",
      "[iteration 1001] loss: 99.9684\n",
      "[iteration 1201] loss: 99.7263\n",
      "[iteration 1401] loss: 98.8524\n",
      "[iteration 1601] loss: 103.7788\n",
      "[iteration 1801] loss: 97.9760\n",
      "[iteration 2001] loss: 99.1748\n",
      "[iteration 2201] loss: 100.2387\n",
      "[iteration 2401] loss: 100.4673\n",
      "[iteration 2601] loss: 101.3497\n",
      "[iteration 2801] loss: 97.7119\n",
      "[iteration 3001] loss: 99.4619\n",
      "[iteration 3201] loss: 98.8285\n",
      "[iteration 3401] loss: 98.7910\n",
      "[iteration 3601] loss: 99.8696\n",
      "[iteration 3801] loss: 99.4081\n",
      "[iteration 4001] loss: 99.4065\n",
      "[iteration 4201] loss: 98.7188\n",
      "[iteration 4401] loss: 99.6752\n",
      "[iteration 4601] loss: 98.1057\n",
      "[iteration 4801] loss: 99.3412\n"
     ]
    }
   ],
   "source": [
    "FA_model = FA(Y,20, 30, n_factors)\n",
    "losses, estimates = FA_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      2\u001b[0m FA_model \u001b[38;5;241m=\u001b[39m FA(Y,\u001b[38;5;241m134\u001b[39m, \u001b[38;5;241m2701\u001b[39m, n_factors)\n\u001b[0;32m----> 3\u001b[0m losses, estimates \u001b[38;5;241m=\u001b[39m \u001b[43mFA_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 116\u001b[0m, in \u001b[0;36mFA.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#for j in enumerate(self.train_dataloader):\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# calculate the loss and take a gradient step\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m#    test_loss.append(elbo.loss(self.model, guide, test_data))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/elbo.py:237\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/enum.py:60\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     58\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m unwrapped_guide\u001b[38;5;241m.\u001b[39mget_traces()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detach:\n\u001b[1;32m     64\u001b[0m         guide_trace\u001b[38;5;241m.\u001b[39mdetach_()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py:216\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Trace:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py:191\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    193\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/nn/module.py:450\u001b[0m, in \u001b[0;36mPyroModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyro_context:\n\u001b[0;32m--> 450\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    452\u001b[0m         pyro\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidate_poutine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyro_context\u001b[38;5;241m.\u001b[39mactive\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m _is_module_local_param_enabled()\n\u001b[1;32m    455\u001b[0m     ):\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_module_local_param_usage()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/autoguide/guides.py:241\u001b[0m, in \u001b[0;36mAutoGuideList.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# if we've never run the model before, do so now so we can inspect the model structure\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprototype_trace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_prototype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# create all plates\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_plates(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/autoguide/guides.py:157\u001b[0m, in \u001b[0;36mAutoGuide._setup_prototype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_prototype\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# run the model so we can inspect its structure\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     model \u001b[38;5;241m=\u001b[39m poutine\u001b[38;5;241m.\u001b[39mblock(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prototype_hide_fn)\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprototype_trace \u001b[38;5;241m=\u001b[39m \u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoutine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaster \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaster()\u001b[38;5;241m.\u001b[39m_check_prototype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprototype_trace)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/messenger.py:32\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_context_wrap\u001b[39m(\n\u001b[1;32m     26\u001b[0m     context: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessenger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     fn: Callable,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     30\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py:216\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Trace:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/trace_messenger.py:191\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    193\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/messenger.py:32\u001b[0m, in \u001b[0;36m_context_wrap\u001b[0;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_context_wrap\u001b[39m(\n\u001b[1;32m     26\u001b[0m     context: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessenger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     fn: Callable,\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     30\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mFA.model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m         batch_size\u001b[38;5;241m.\u001b[39mappend(ind)\n\u001b[1;32m     45\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_size[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpyro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# sample factor matrix with Normal prior distribution\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         Z \u001b[38;5;241m=\u001b[39m pyro\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m\"\u001b[39m, pyro\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# estimate for Y\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/subsample_messenger.py:89\u001b[0m, in \u001b[0;36mSubsampleMessenger.__init__\u001b[0;34m(self, name, size, subsample_size, subsample, dim, use_cuda, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     81\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     device: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     full_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubsample_size, subsample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_subsample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubsample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, full_size, dim, device)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m subsample\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/subsample_messenger.py:114\u001b[0m, in \u001b[0;36mSubsampleMessenger._subsample\u001b[0;34m(name, size, subsample_size, subsample, use_cuda, device)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m subsample_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m subsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# This is PyTorch convention for \"arbitrary size\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     subsample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_factors = 5\n",
    "FA_model = FA(Y,134, 2701, n_factors)\n",
    "losses, estimates = FA_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2835])\n",
      "Z torch.Size([128, 7])\n",
      "yyy \n",
      "Z torch.Size([128, 7])\n",
      "yyy \n",
      "Z torch.Size([128, 7])\n",
      "yyy \n",
      "Z torch.Size([128, 7])\n",
      "yyy \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thien/.local/lib/python3.8/site-packages/pyro/util.py:365: UserWarning: Found plate statements in guide but not model: {'latent factors', 'feature2', 'feature1'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m n_factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[1;32m      2\u001b[0m FA_model \u001b[38;5;241m=\u001b[39m FA(Y, \u001b[38;5;241m134\u001b[39m, \u001b[38;5;241m2701\u001b[39m, n_factors)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 3\u001b[0m losses, estimates \u001b[38;5;241m=\u001b[39m \u001b[43mFA_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 161\u001b[0m, in \u001b[0;36mFA.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# calculate the loss and take a gradient step\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m    163\u001b[0m     batch_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    149\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_trace, guide_trace \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_traces(model, guide, args, kwargs):\n\u001b[1;32m    141\u001b[0m     loss_particle, surrogate_loss_particle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_differentiable_loss_particle(\n\u001b[1;32m    142\u001b[0m         model_trace, guide_trace\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_particle \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/elbo.py:237\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[0;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[1;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/infer/enum.py:75\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[1;32m     72\u001b[0m guide_trace \u001b[38;5;241m=\u001b[39m prune_subsample_sites(guide_trace)\n\u001b[1;32m     73\u001b[0m model_trace \u001b[38;5;241m=\u001b[39m prune_subsample_sites(model_trace)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mmodel_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m guide_trace\u001b[38;5;241m.\u001b[39mcompute_score_parts()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/poutine/trace_struct.py:276\u001b[0m, in \u001b[0;36mTrace.compute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while computing log_prob at site \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    272\u001b[0m             name, exc_value, shapes\n\u001b[1;32m    273\u001b[0m         )\n\u001b[1;32m    274\u001b[0m     )\u001b[38;5;241m.\u001b[39mwith_traceback(traceback) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    275\u001b[0m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munscaled_log_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_p\n\u001b[0;32m--> 276\u001b[0m log_p \u001b[38;5;241m=\u001b[39m \u001b[43mscale_and_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_prob\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_p\n\u001b[1;32m    278\u001b[0m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_prob_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_p\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyro/distributions/util.py:328\u001b[0m, in \u001b[0;36mscale_and_mask\u001b[0;34m(tensor, scale, mask)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(tensor)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "\n",
    "n_factors = 7\n",
    "FA_model = FA(Y, 134, 2701, n_factors).cuda()\n",
    "losses, estimates = FA_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'obs1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m correlations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent_factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_factors):\n\u001b[0;32m----> 3\u001b[0m     correlations\u001b[38;5;241m.\u001b[39mappend(spearmanr(Y1[:,latent_factor], \u001b[43mestimates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobs1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[:,latent_factor], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      4\u001b[0m correlations\n",
      "\u001b[0;31mKeyError\u001b[0m: 'obs1'"
     ]
    }
   ],
   "source": [
    "correlations = []\n",
    "for latent_factor in range(n_factors):\n",
    "    correlations.append(spearmanr(W1_in[:,latent_factor], estimates[\"w1_hat\"].detach().numpy()[:,latent_factor], axis=0))\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\\n,\n",
    "n_obs = 100\n",
    "n_features1 = 20\n",
    "n_features2 = 30\n",
    "n_factors = 20\n",
    "\n",
    "torch.manual_seed(2024)\n",
    "Z_in = torch.randn(n_obs, n_factors)\n",
    "W1_in = torch.randn(n_features1, n_factors)\n",
    "W2_in = torch.randn(n_features2, n_factors)\n",
    "\n",
    "with pyro.plate(\"features1\", n_features1):\n",
    "    precision1 = pyro.sample(\"precision1\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "    residuals1 = pyro.sample(\"scale1\", pyro.distributions.Normal(0., 1./(precision1)))\n",
    "with pyro.plate(\"features2\", n_features2):\n",
    "    precision2 = pyro.sample(\"precision2\", pyro.distributions.Gamma(torch.tensor(10.), torch.tensor(10.)))\n",
    "    residuals2 = pyro.sample(\"scale2\", pyro.distributions.Normal(0., 1./(precision2)))\n",
    "\n",
    "# create observated values from the simulated factor and weight matrix with some random noise\\n,\n",
    "Y1 = torch.matmul(Z_in, W1_in.t()) + residuals1\n",
    "Y2 = torch.matmul(Z_in, W2_in.t()) + residuals2\n",
    "#print(Y.shape)\\n,\n",
    "#print(Y)\\n,\n",
    "\n",
    "Y = torch.cat((Y1.T, Y2.T)).T\n",
    "#train_size = int(0.2*Y.shape[0])\n",
    "#train_data = Y[:train_size,:]\n",
    "#test_data = Y[train_size:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "saved_model_dict = torch.load(\"/mnt/storage/thien/FA_model.pt\")\n",
    "FA_model.load_state_dict(saved_model_dict['model'])\n",
    "guide = saved_model_dict['guide']\n",
    "pyro.get_param_store().load(\"/mnt/storage/thien/FA_model_params.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'w1_hat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m correlations_spearman_temp \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m latent_factor2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_factors):\n\u001b[0;32m----> 7\u001b[0m     correlations_pearson_temp\u001b[38;5;241m.\u001b[39mappend(pearsonr(W1_in[:,latent_factor], \u001b[43mestimates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw1_hat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[:,latent_factor2]))\n\u001b[1;32m      8\u001b[0m     correlations_spearman_temp\u001b[38;5;241m.\u001b[39mappend(spearmanr(W1_in[:,latent_factor], estimates[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw1_hat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[:,latent_factor2], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      9\u001b[0m correlations_pearson\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmax\u001b[39m(correlations_pearson_temp))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'w1_hat'"
     ]
    }
   ],
   "source": [
    "correlations_pearson = []\n",
    "correlations_spearman = []\n",
    "for latent_factor in range(n_factors):\n",
    "    correlations_pearson_temp = []\n",
    "    correlations_spearman_temp = []\n",
    "    for latent_factor2 in range(n_factors):\n",
    "        correlations_pearson_temp.append(pearsonr(W1_in[:,latent_factor], estimates[\"w1_hat\"].detach().numpy()[:,latent_factor2]))\n",
    "        correlations_spearman_temp.append(spearmanr(W1_in[:,latent_factor], estimates[\"w1_hat\"].detach().numpy()[:,latent_factor2], axis=0))\n",
    "    correlations_pearson.append(max(correlations_pearson_temp))\n",
    "    correlations_spearman.append(max(correlations_spearman_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PearsonRResult(statistic=0.4038948812760812, pvalue=0.07737831930753168),\n",
       " PearsonRResult(statistic=0.47071420010067627, pvalue=0.03619766659962283),\n",
       " PearsonRResult(statistic=0.5220001087235753, pvalue=0.0182336410265067),\n",
       " PearsonRResult(statistic=0.4324443797043427, pvalue=0.05687121231608197),\n",
       " PearsonRResult(statistic=0.4828671439209355, pvalue=0.03104114380806438),\n",
       " PearsonRResult(statistic=0.46900320810693163, pvalue=0.03697415778941149),\n",
       " PearsonRResult(statistic=0.6147403039527163, pvalue=0.003922521747556146),\n",
       " PearsonRResult(statistic=0.4274349716878614, pvalue=0.06013154248417405),\n",
       " PearsonRResult(statistic=0.5425697167025167, pvalue=0.013446071308511547),\n",
       " PearsonRResult(statistic=0.5435065790120842, pvalue=0.013254963722082453),\n",
       " PearsonRResult(statistic=0.6147888038061001, pvalue=0.003918891204804904),\n",
       " PearsonRResult(statistic=0.4416069817841369, pvalue=0.051257365946899155),\n",
       " PearsonRResult(statistic=0.31504222144715494, pvalue=0.17607291855098597),\n",
       " PearsonRResult(statistic=0.5650394885423163, pvalue=0.009431755056118303),\n",
       " PearsonRResult(statistic=nan, pvalue=nan),\n",
       " PearsonRResult(statistic=0.6496841105975766, pvalue=0.001932837636290756),\n",
       " PearsonRResult(statistic=0.5524488844015804, pvalue=0.011538798817758726),\n",
       " PearsonRResult(statistic=0.4811408871822462, pvalue=0.031736268555339404),\n",
       " PearsonRResult(statistic=0.6506294157805768, pvalue=0.0018939050537961854),\n",
       " PearsonRResult(statistic=0.5720042320165807, pvalue=0.008408071730186751),\n",
       " PearsonRResult(statistic=0.517639350776965, pvalue=0.01940505984955512),\n",
       " PearsonRResult(statistic=0.4260831558362887, pvalue=0.061035127741895526),\n",
       " PearsonRResult(statistic=0.5588591634870103, pvalue=0.01042299196111059),\n",
       " PearsonRResult(statistic=0.46223695140062127, pvalue=0.04017244136291743),\n",
       " PearsonRResult(statistic=0.5278081915024329, pvalue=0.01676190848304904),\n",
       " PearsonRResult(statistic=0.5864215951090982, pvalue=0.006575726554350759),\n",
       " PearsonRResult(statistic=0.6751673514556391, pvalue=0.0010894716287057635),\n",
       " PearsonRResult(statistic=0.4977199128432799, pvalue=0.025540937402209123),\n",
       " PearsonRResult(statistic=0.5325055505723025, pvalue=0.01564259498841084),\n",
       " PearsonRResult(statistic=0.6024378966277896, pvalue=0.004937987394983246),\n",
       " PearsonRResult(statistic=0.5510853594260556, pvalue=0.011788175960121767),\n",
       " PearsonRResult(statistic=0.5300056522227129, pvalue=0.016230560016337325),\n",
       " PearsonRResult(statistic=0.6327355410510527, pvalue=0.002753105214493686),\n",
       " PearsonRResult(statistic=0.52991082231291, pvalue=0.01625320755819807),\n",
       " PearsonRResult(statistic=0.6524401231860386, pvalue=0.0018211624942323743),\n",
       " PearsonRResult(statistic=0.7220750773474971, pvalue=0.0003245709080839487),\n",
       " PearsonRResult(statistic=0.7156579439226084, pvalue=0.0003883195709569975),\n",
       " PearsonRResult(statistic=0.4895245550131292, pvalue=0.028471350752481136),\n",
       " PearsonRResult(statistic=0.43736214282232283, pvalue=0.05380291036806793),\n",
       " PearsonRResult(statistic=0.4745019092811067, pvalue=0.034523858504239065),\n",
       " PearsonRResult(statistic=nan, pvalue=nan),\n",
       " PearsonRResult(statistic=0.6761673953805234, pvalue=0.0010640619034440217),\n",
       " PearsonRResult(statistic=0.4945378309585007, pvalue=0.02664904159788084),\n",
       " PearsonRResult(statistic=0.5553647296081511, pvalue=0.011019837509441726),\n",
       " PearsonRResult(statistic=0.4075508430740013, pvalue=0.07448319522200017),\n",
       " PearsonRResult(statistic=0.6937969089129441, pvalue=0.0006915361230837276),\n",
       " PearsonRResult(statistic=0.523290616887153, pvalue=0.017898034497685077),\n",
       " PearsonRResult(statistic=0.6073831174844738, pvalue=0.004506485531405644),\n",
       " PearsonRResult(statistic=0.44876643557688123, pvalue=0.04717380079564478),\n",
       " PearsonRResult(statistic=0.4667829888600241, pvalue=0.03800098012500714),\n",
       " PearsonRResult(statistic=0.5211838728208448, pvalue=0.018448492133429),\n",
       " PearsonRResult(statistic=0.5948247881305038, pvalue=0.0056684905496943334),\n",
       " PearsonRResult(statistic=0.5060977546002929, pvalue=0.02279683783459018),\n",
       " PearsonRResult(statistic=0.4077790289049447, pvalue=0.07430521566486951),\n",
       " PearsonRResult(statistic=0.5275958393325418, pvalue=0.016813985041033493),\n",
       " PearsonRResult(statistic=0.6559344918454734, pvalue=0.001687386029706595),\n",
       " PearsonRResult(statistic=0.43937236213879427, pvalue=0.05258568209900906),\n",
       " PearsonRResult(statistic=0.3499209694095198, pvalue=0.13042755540251014),\n",
       " PearsonRResult(statistic=0.467245891385342, pvalue=0.03778508841261648),\n",
       " PearsonRResult(statistic=0.6792971033370878, pvalue=0.0009877443412130442),\n",
       " PearsonRResult(statistic=0.4224076123884894, pvalue=0.06354387538790243),\n",
       " PearsonRResult(statistic=0.5851587089902623, pvalue=0.006721824886859265),\n",
       " PearsonRResult(statistic=0.6418663697838124, pvalue=0.0022812534556708856),\n",
       " PearsonRResult(statistic=0.49106668056461406, pvalue=0.02790066541378692),\n",
       " PearsonRResult(statistic=0.3398758492605762, pvalue=0.14260390922520597),\n",
       " PearsonRResult(statistic=0.7002057711326982, pvalue=0.0005869210341033217),\n",
       " PearsonRResult(statistic=0.3517186671416071, pvalue=0.12832877668198325),\n",
       " PearsonRResult(statistic=0.644513032479663, pvalue=0.0021578641478848617),\n",
       " PearsonRResult(statistic=0.7405207119839158, pvalue=0.00018849825567781594),\n",
       " PearsonRResult(statistic=0.38512145685012855, pvalue=0.09357838430611523),\n",
       " PearsonRResult(statistic=0.5697977139229421, pvalue=0.008722063166336457),\n",
       " PearsonRResult(statistic=0.48066615340011726, pvalue=0.03192955292754389),\n",
       " PearsonRResult(statistic=0.6043813930622715, pvalue=0.004764512564894535),\n",
       " PearsonRResult(statistic=0.6161628349970422, pvalue=0.0038171858780609324),\n",
       " PearsonRResult(statistic=0.5119641331993305, pvalue=0.02101860693333063),\n",
       " PearsonRResult(statistic=0.4148188133361419, pvalue=0.06896880079833072),\n",
       " PearsonRResult(statistic=0.415391207559996, pvalue=0.06854790406288103),\n",
       " PearsonRResult(statistic=0.44209948157640044, pvalue=0.050968091843688626),\n",
       " PearsonRResult(statistic=0.6059004801406868, pvalue=0.004632449936903644),\n",
       " PearsonRResult(statistic=0.34484210504070584, pvalue=0.13648823829244713),\n",
       " PearsonRResult(statistic=0.42932958883965205, pvalue=0.058882224961900424),\n",
       " PearsonRResult(statistic=0.5352467225366503, pvalue=0.015017665835729964),\n",
       " PearsonRResult(statistic=0.6175175468044325, pvalue=0.0037190614075508625),\n",
       " PearsonRResult(statistic=0.6681982648922141, pvalue=0.0012811497572520363),\n",
       " PearsonRResult(statistic=0.5163128265900999, pvalue=0.019773057029660895),\n",
       " PearsonRResult(statistic=0.4866551115540882, pvalue=0.02955762383204067),\n",
       " PearsonRResult(statistic=0.6013085789087073, pvalue=0.0050411598696097175),\n",
       " PearsonRResult(statistic=0.6984623370756021, pvalue=0.0006139530504254338),\n",
       " PearsonRResult(statistic=0.527201804725507, pvalue=0.01691096013000237),\n",
       " PearsonRResult(statistic=0.5355727221897337, pvalue=0.014944699908989374),\n",
       " PearsonRResult(statistic=0.43930576820879086, pvalue=0.0526256663958799),\n",
       " PearsonRResult(statistic=0.6445787386526844, pvalue=0.002154873329381147),\n",
       " PearsonRResult(statistic=0.7494474980415355, pvalue=0.0001425821296429401),\n",
       " PearsonRResult(statistic=0.428489638264503, pvalue=0.059433640081111926),\n",
       " PearsonRResult(statistic=0.5165557116915437, pvalue=0.01970526457518624),\n",
       " PearsonRResult(statistic=0.38190297035987913, pvalue=0.09658767398807983),\n",
       " PearsonRResult(statistic=0.3727013569211103, pvalue=0.10558062128552362),\n",
       " PearsonRResult(statistic=0.4928310545858925, pvalue=0.02725881589295251),\n",
       " PearsonRResult(statistic=0.5522728165053447, pvalue=0.01157075785792441),\n",
       " PearsonRResult(statistic=0.6864453798063517, pvalue=0.0008305806246009329)]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SignificanceResult(statistic=0.406498964473461, pvalue=0.07530775045002563),\n",
       " SignificanceResult(statistic=0.4922261406881995, pvalue=0.0274775494744418),\n",
       " SignificanceResult(statistic=0.586778054718701, pvalue=0.006534963878379691),\n",
       " SignificanceResult(statistic=0.45064185315494376, pvalue=0.046146603768128985),\n",
       " SignificanceResult(statistic=0.38718631774649104, pvalue=0.09168426162995048),\n",
       " SignificanceResult(statistic=0.5273259081822316, pvalue=0.016880369071840817),\n",
       " SignificanceResult(statistic=0.45850787319122654, pvalue=0.04202437436925066),\n",
       " SignificanceResult(statistic=0.3726457788260945, pvalue=0.10563671808163036),\n",
       " SignificanceResult(statistic=0.4514390405181792, pvalue=0.04571521636184646),\n",
       " SignificanceResult(statistic=0.534582686735198, pvalue=0.015167177370541974),\n",
       " SignificanceResult(statistic=0.5140973885633965, pvalue=0.02040003269625392),\n",
       " SignificanceResult(statistic=0.46614722254101953, pvalue=0.03829905267465168),\n",
       " SignificanceResult(statistic=0.36708390153018267, pvalue=0.11136036769043006),\n",
       " SignificanceResult(statistic=0.6192451031864737, pvalue=0.003596973031346407),\n",
       " SignificanceResult(statistic=nan, pvalue=nan),\n",
       " SignificanceResult(statistic=0.6304483256379365, pvalue=0.002883242259494056),\n",
       " SignificanceResult(statistic=0.46614722254101953, pvalue=0.03829905267465168),\n",
       " SignificanceResult(statistic=0.4086966687324073, pvalue=0.07359268116874716),\n",
       " SignificanceResult(statistic=0.5454662629343903, pvalue=0.012862296464446166),\n",
       " SignificanceResult(statistic=0.4233120822563785, pvalue=0.06291943707356738),\n",
       " SignificanceResult(statistic=0.4208931560720564, pvalue=0.06459989501740675),\n",
       " SignificanceResult(statistic=0.4398866776619857, pvalue=0.052277657548956434),\n",
       " SignificanceResult(statistic=0.48779492637419375, pvalue=0.029122300872083492),\n",
       " SignificanceResult(statistic=0.37550213752729766, pvalue=0.10278156873746194),\n",
       " SignificanceResult(statistic=0.5205504938591714, pvalue=0.018616600564267234),\n",
       " SignificanceResult(statistic=0.5209880722517277, pvalue=0.018500330594991184),\n",
       " SignificanceResult(statistic=0.549418917013625, pvalue=0.012098857376301179),\n",
       " SignificanceResult(statistic=0.49356764739637365, pvalue=0.02699432216450656),\n",
       " SignificanceResult(statistic=0.5540452808182447, pvalue=0.011252277239145455),\n",
       " SignificanceResult(statistic=0.5119463534648049, pvalue=0.021023824421871687),\n",
       " SignificanceResult(statistic=0.4446735839139051, pvalue=0.049476431277042224),\n",
       " SignificanceResult(statistic=0.534582686735198, pvalue=0.015167177370541974),\n",
       " SignificanceResult(statistic=0.49356764739637365, pvalue=0.02699432216450656),\n",
       " SignificanceResult(statistic=0.42886296759696624, pvalue=0.059188073993570095),\n",
       " SignificanceResult(statistic=0.3779644730092272, pvalue=0.1003656219472864),\n",
       " SignificanceResult(statistic=0.5194749763098755, pvalue=0.018904859310062964),\n",
       " SignificanceResult(statistic=0.7587244291460039, pvalue=0.0001053744304829371),\n",
       " SignificanceResult(statistic=0.4527928882535354, pvalue=0.04498971428291271),\n",
       " SignificanceResult(statistic=0.4302070197183234, pvalue=0.05831035511940855),\n",
       " SignificanceResult(statistic=0.5028754937363049, pvalue=0.023823135120565768),\n",
       " SignificanceResult(statistic=nan, pvalue=nan),\n",
       " SignificanceResult(statistic=0.3779644730092272, pvalue=0.1003656219472864),\n",
       " SignificanceResult(statistic=0.40514704312155797, pvalue=0.07637747488277365),\n",
       " SignificanceResult(statistic=0.49356764739637365, pvalue=0.02699432216450656),\n",
       " SignificanceResult(statistic=0.42718135564130577, pvalue=0.06030028878124179),\n",
       " SignificanceResult(statistic=0.33341044028170064, pvalue=0.15085014768678964),\n",
       " SignificanceResult(statistic=0.4592459935493103, pvalue=0.041652674074218604),\n",
       " SignificanceResult(statistic=0.5553478981324771, pvalue=0.011022777906248992),\n",
       " SignificanceResult(statistic=0.3779644730092272, pvalue=0.1003656219472864),\n",
       " SignificanceResult(statistic=0.44680414277158603, pvalue=0.048267283459834484),\n",
       " SignificanceResult(statistic=0.550664985239454, pvalue=0.011865932192454421),\n",
       " SignificanceResult(statistic=0.6071716048037112, pvalue=0.0045242805404841595),\n",
       " SignificanceResult(statistic=0.5265243840129893, pvalue=0.017078725453905375),\n",
       " SignificanceResult(statistic=0.5517405027887498, pvalue=0.011667817102973961),\n",
       " SignificanceResult(statistic=0.5209880722517277, pvalue=0.018500330594991184),\n",
       " SignificanceResult(statistic=0.4388111601126899, pvalue=0.052923365319293814),\n",
       " SignificanceResult(statistic=0.3726457788260945, pvalue=0.10563671808163036),\n",
       " SignificanceResult(statistic=0.3289739610678142, pvalue=0.15669707070346883),\n",
       " SignificanceResult(statistic=0.46614722254101953, pvalue=0.03829905267465168),\n",
       " SignificanceResult(statistic=0.46614722254101953, pvalue=0.03829905267465168),\n",
       " SignificanceResult(statistic=0.5334567044507211, pvalue=0.015423430643509138),\n",
       " SignificanceResult(statistic=0.49228864123659316, pvalue=0.027454885808409303),\n",
       " SignificanceResult(statistic=0.5710998186760744, pvalue=0.00853563668792637),\n",
       " SignificanceResult(statistic=0.5807794766197366, pvalue=0.0072492042859838846),\n",
       " SignificanceResult(statistic=0.4475013440996002, pvalue=0.047876566348541344),\n",
       " SignificanceResult(statistic=0.5563530223940975, pvalue=0.010848297737485376),\n",
       " SignificanceResult(statistic=0.5176502034449428, pvalue=0.01940207184682056),\n",
       " SignificanceResult(statistic=0.5988270930040588, pvalue=0.005274117177544478),\n",
       " SignificanceResult(statistic=0.5140973885633965, pvalue=0.02040003269625392),\n",
       " SignificanceResult(statistic=0.40509006305224704, pvalue=0.07642280821612528),\n",
       " SignificanceResult(statistic=0.5909217848455894, pvalue=0.006076131576042608),\n",
       " SignificanceResult(statistic=0.3616678482499762, pvalue=0.11714554793598947),\n",
       " SignificanceResult(statistic=0.5538915378873415, pvalue=0.011279617659311337),\n",
       " SignificanceResult(statistic=0.5950558413432521, pvalue=0.00564508238513909),\n",
       " SignificanceResult(statistic=0.5321637605508759, pvalue=0.01572195790664025),\n",
       " SignificanceResult(statistic=0.3997609307859517, pvalue=0.08075188841722025),\n",
       " SignificanceResult(statistic=0.4065456336338157, pvalue=0.07527102342538708),\n",
       " SignificanceResult(statistic=0.4398866776619857, pvalue=0.052277657548956434),\n",
       " SignificanceResult(statistic=0.5805124120675238, pvalue=0.007282431013253435),\n",
       " SignificanceResult(statistic=0.46032151109860603, pvalue=0.04111562305087688),\n",
       " SignificanceResult(statistic=0.48720944983100123, pvalue=0.029345275016287045),\n",
       " SignificanceResult(statistic=0.3779644730092272, pvalue=0.1003656219472864),\n",
       " SignificanceResult(statistic=0.43938830637703685, pvalue=0.05257611230794037),\n",
       " SignificanceResult(statistic=0.6808026087042468, pvalue=0.0009527111929523322),\n",
       " SignificanceResult(statistic=0.48620416304875475, pvalue=0.0297312556136645),\n",
       " SignificanceResult(statistic=0.49356764739637365, pvalue=0.02699432216450656),\n",
       " SignificanceResult(statistic=0.5237770465070588, pvalue=0.01777282383158352),\n",
       " SignificanceResult(statistic=0.6238001785915689, pvalue=0.0032909014939917418),\n",
       " SignificanceResult(statistic=0.4882849673802971, pvalue=0.028936697614911),\n",
       " SignificanceResult(statistic=0.5097953183662133, pvalue=0.021662689802968895),\n",
       " SignificanceResult(statistic=0.4667746163943809, pvalue=0.03800489373152416),\n",
       " SignificanceResult(statistic=0.6521879230737275, pvalue=0.0018311517060290755),\n",
       " SignificanceResult(statistic=0.6603677752676265, pvalue=0.0015296116662351592),\n",
       " SignificanceResult(statistic=0.37171879927677587, pvalue=0.10657553527201766),\n",
       " SignificanceResult(statistic=0.5024229157307045, pvalue=0.023970164680947363),\n",
       " SignificanceResult(statistic=0.5181017117406379, pvalue=0.01927808541111822),\n",
       " SignificanceResult(statistic=0.5505163299850073, pvalue=0.011893527924786576),\n",
       " SignificanceResult(statistic=0.4208931560720564, pvalue=0.06459989501740675),\n",
       " SignificanceResult(statistic=0.5960478502118874, pvalue=0.005545483087449475),\n",
       " SignificanceResult(statistic=0.5974747675275742, pvalue=0.005404758463644631)]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.matmul(torch.Tensor(estimates[\"Z\"].detach().numpy()),torch.Tensor(estimates[\"W1\"].detach().numpy()).t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_pearson = []\n",
    "correlations_spearman = []\n",
    "for observation in range(n_obs):\n",
    "    correlations_pearson_temp = []\n",
    "    correlations_spearman_temp = []\n",
    "    for observation2 in range(n_obs):\n",
    "        correlations_pearson_temp.append(pearsonr(Y1[observation,:], k[observation2,:]))\n",
    "        correlations_spearman_temp.append(spearmanr(Y1[observation,:], k[observation2,:], axis=0))\n",
    "    correlations_pearson.append(max(correlations_pearson_temp))\n",
    "    correlations_spearman.append(max(correlations_spearman_temp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
